"""
æ ¹æ®è¯¾ç¨‹å†…å®¹ç”Ÿæˆé¢˜ç›®è„šæœ¬ (Producer)
è¯»å– courses/ ç›®å½•ä¸‹çš„ Markdown æ–‡ä»¶ï¼Œä½¿ç”¨ DeepSeek ç”Ÿæˆé¢˜ç›® JSONï¼Œ
ç”Ÿæˆçš„ JSON æ–‡ä»¶å¯ç›´æ¥è¢« import_questions.py (Consumer) è„šæœ¬ä½¿ç”¨ã€‚
"""
import os
import sys
import json
import asyncio
import glob
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
from openai import AsyncOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
BASE_DIR = Path(__file__).resolve().parent.parent
BACKEND_DIR = BASE_DIR / 'src' / 'backend'
ENV_PATH = BACKEND_DIR / '.env'

if ENV_PATH.exists():
    load_dotenv(ENV_PATH)
else:
    print(f"Warning: .env file not found at {ENV_PATH}")

API_KEY = os.getenv("LLM_API_KEY")
BASE_URL = os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
MODEL = os.getenv("LLM_MODEL", "gpt-3.5-turbo")

if not API_KEY:
    print("Error: LLM_API_KEY environment variable not set.")
    sys.exit(1)

client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)

OUTPUT_DIR = Path(__file__).parent / "data" / "output"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•™è‚²å‡ºé¢˜ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·æä¾›çš„è¯¾ç¨‹å†…å®¹ï¼Œç”Ÿæˆç›¸å…³çš„å•é¡¹é€‰æ‹©é¢˜ã€‚
è¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„ JSON æ ¼å¼æ•°ç»„ï¼Œä¸è¦åŒ…å« markdown æ ‡è®°æˆ–å…¶ä»–æ–‡æœ¬ã€‚

æ¯ä¸ªé¢˜ç›®çš„ JSON ç»“æ„å¦‚ä¸‹ï¼š
{
    "content": "é¢˜ç›®å†…å®¹",
    "question_type": "single_choice",
    "options": {
        "A": "é€‰é¡¹Aå†…å®¹",
        "B": "é€‰é¡¹Bå†…å®¹",
        "C": "é€‰é¡¹Cå†…å®¹",
        "D": "é€‰é¡¹Då†…å®¹"
    },
    "correct_answer": "A",  # å¿…é¡»æ˜¯é€‰é¡¹çš„ Key (A, B, C, D)
    "explanation": "ç­”æ¡ˆè§£æ",
    "difficulty": 1,  # 1-3, 1ä¸ºç®€å•, 2ä¸ºä¸­ç­‰, 3ä¸ºå›°éš¾
    "knowledge_points": ["çŸ¥è¯†ç‚¹1", "çŸ¥è¯†ç‚¹2"]
}

è¦æ±‚ï¼š
1. é¢˜ç›®è¦æœ‰é’ˆå¯¹æ€§ï¼Œè€ƒå¯Ÿè¯¾ç¨‹ä¸­çš„æ ¸å¿ƒæ¦‚å¿µã€‚
2. é€‰é¡¹è¦æœ‰å¹²æ‰°æ€§ã€‚
3. ç”Ÿæˆ 3-5 é“é¢˜ç›®ã€‚
4. è¿”å›ä»…ä»…æ˜¯ä¸€ä¸ª JSON æ•°ç»„ã€‚
"""

async def generate_questions_for_text(text: str, context_info: str) -> List[Dict]:
    """è°ƒç”¨ LLM ç”Ÿæˆé¢˜ç›®"""
    print(f"   Generating questions for: {context_info}...")
    
    try:
        response = await client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": f"ã€è¯¾ç¨‹å†…å®¹ã€‘\n{text[:4000]}..."} # æˆªå–å‰4000å­—ç¬¦é¿å…è¶…é•¿
            ],
            stream=False
        )
        
        content = response.choices[0].message.content.strip()
        
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ Markdown ä»£ç å—æ ‡è®°
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        
        return json.loads(content.strip())
        
    except Exception as e:
        print(f"   Error generating questions: {e}")
        return []

async def process_course_folder(course_dir: Path):
    """å¤„ç†å•ä¸ªè¯¾ç¨‹ç›®å½•"""
    course_code = course_dir.name
    print(f"ğŸ“¦ Processing course: {course_code}")
    
    # è·å–æ‰€æœ‰ Markdown æ–‡ä»¶
    md_files = sorted(course_dir.glob("*.md"))
    
    if not md_files:
        print(f"   No markdown files found in {course_dir}")
        return

    total_generated = 0
    
    for md_file in md_files:
        # è·³è¿‡éç« èŠ‚æ–‡ä»¶
        if md_file.name.lower() in ['readme.md', 'summary.md']:
            continue
            
        print(f"   ğŸ“„ Reading chapter: {md_file.name}")
        
        try:
            text_content = md_file.read_text(encoding='utf-8')
            if len(text_content.strip()) < 100:
                print("   Skipping: content too short")
                continue
                
            questions = await generate_questions_for_text(text_content, f"{course_code} - {md_file.name}")
            
            if questions:
                # æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
                for q in questions:
                    q["metadata"] = {
                        "source_file": md_file.name,
                        "generated_by": "deepseek-v3"
                    }
                
                # ä¿å­˜åˆ°è¾“å‡ºç›®å½•
                output_filename = f"{course_code}_{md_file.stem}_questions.json"
                output_path = OUTPUT_DIR / output_filename
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(questions, f, ensure_ascii=False, indent=2)
                
                print(f"   âœ… Saved {len(questions)} questions to {output_path.name}")
                total_generated += len(questions)
            else:
                print("   âš ï¸ No questions generated")
                
        except Exception as e:
            print(f"   Error processing file {md_file.name}: {e}")

    print(f"ğŸ‰ Course {course_code} processing complete. Total questions: {total_generated}")

async def main():
    # è¯¾ç¨‹æ ¹ç›®å½•
    courses_root = BASE_DIR / "courses"
    
    if not courses_root.exists():
        print(f"Error: Courses directory not found at {courses_root}")
        # å°è¯• fallback åˆ° courses
        courses_root = BASE_DIR / "courses"
        if not courses_root.exists():
             print(f"Error: Neither 'learning_courses' nor 'courses' directory found.")
             return
        print(f"Fallback to: {courses_root}")

    # éå†æ¯ä¸ªè¯¾ç¨‹æ–‡ä»¶å¤¹
    tasks = []
    for course_dir in courses_root.iterdir():
        if course_dir.is_dir() and not course_dir.name.startswith('.'):
            # ä¸²è¡Œå¤„ç†æ¯ä¸ªè¯¾ç¨‹ï¼Œé¿å…å¹¶å‘è¿‡é«˜
            await process_course_folder(course_dir)

if __name__ == "__main__":
    asyncio.run(main())