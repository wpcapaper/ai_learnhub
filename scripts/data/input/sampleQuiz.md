单选题 （每题1分，共30道题）
1、 [单选] 在大模型技术选型路径中，按照成本效益原则，应优先尝试的最低成本方案是什么？
 A：检索增强生成(RAG)
 B：微调(Fine-tuning)
 C：提示词工程(Prompt Engineering)
 D：全量预训练
正确答案：C 你的答案：C
 解析：根据教程中的技术选型路径，应遵循成本效益原则，从低到高依次为"提示词工程(Prompt Engineering) -> 检索增强生成(RAG) -> 微调(Fine-tuning)"。应先尝试成本最低的"上下文优化"策略，通过精心设计提示词来引导模型。来源：docs/chapter11/04_qwen2.5_qlora.md 第569-571行。

2、 [单选] LoRA(Low-Rank Adaptation)技术中，将低秩矩阵A和B初始化的目的是什么？
 A：矩阵A初始化为全零，矩阵B使用高斯分布随机初始化
 B：矩阵A和B都使用高斯分布随机初始化
 C：矩阵A使用高斯分布随机初始化，矩阵B初始化为全零
 D：矩阵A和B都初始化为全零
正确答案：C 你的答案：C
 解析：LoRA的旁路矩阵有特殊的初始化方式。矩阵A通常使用高斯分布进行随机初始化，而矩阵B则初始化为全零。这样做可以确保在训练开始时，旁路输出为零，微调是从原始的预训练模型状态开始的，保证了训练初期的稳定性。来源：docs/chapter11/02_lora.md 第37-39行。

3、 [单选] OWASP Top 10 for LLM Applications(2025)中，攻击者将恶意指令隐藏在网页或文档中，当RAG系统检索并处理这些内容时模型被劫持的风险被称为什么？
 A：直接提示注入
 B：间接提示注入
 C：系统提示泄露
 D：向量投毒
正确答案：B 你的答案：B
 解析：间接提示注入是指攻击者将恶意指令隐藏在网页、邮件或文档中。当RAG系统或联网模型检索并处理这些内容时，模型被"劫持"执行隐藏指令（如窃取数据）。来源：docs/chapter16/02_threat_modeling_analysis.md 第10-11行。

4、 [单选] 在自注意力机制中，Query、Key、Value这三个向量的来源是什么？
 A：Query来自解码器，Key和Value来自编码器
 B：Query、Key、Value都来自同一个输入序列
 C：Query和Key来自编码器，Value来自解码器
 D：Query、Key、Value分别来自三个不同的输入序列
正确答案：B 你的答案：B
 解析：在自注意力机制中，Query、Key和Value均来源于同一个输入序列。其目的是为了捕捉输入序列内部的依赖关系，重新计算序列中每个词元的表示，使其包含更丰富的上下文信息。来源：docs/chapter4/12_transformer.md 第9-11行。

5、 [单选] Transformer架构的主要优势是什么？
 A：保留了RNN的顺序计算方式
 B：必须按顺序一个词元接一个词元地进行计算
 C：完全基于注意力机制构建，具有出色的并行计算能力
 D：引入了卷积网络来处理序列信息
正确答案：C 你的答案：C
 解析：Transformer抛弃了传统的RNN和卷积网络，整个模型基于注意力机制来构建。它不仅凭借其出色的并行计算能力极大地提升了训练效率，还更有效地捕捉了文本中的长距离依赖关系。来源：docs/chapter4/12_transformer.md 第3-5行。

6、 [单选] 赋予LLM Agent(智能体)过高的权限、功能或自主权，导致其在幻觉或受攻击时造成实质性破坏的风险被称为什么？
 A：系统提示泄露
 B：向量与嵌入弱点
 C：过度代理
 D：数据与模型投毒
正确答案：C 你的答案：C
 解析：过度代理是指赋予LLM Agent(智能体)过高的权限、功能或自主权，导致其在幻觉或受攻击时造成实质性破坏。具体表现形式包括功能过度、权限过度和自主过度。来源：docs/chapter16/02_threat_modeling_analysis.md 第30-33行。

7、 [单选] 标准的Seq2Seq模型存在的核心缺陷是什么？
 A：无法处理中文文本
 B：需要过多的计算资源
 C：信息瓶颈问题
 D：无法进行训练
正确答案：C 你的答案：C
 解析：标准Seq2Seq架构存在一个核心缺陷：信息瓶颈。编码器需要将源序列的所有信息，不论长短，全部压缩成一个固定长度的上下文向量C。这种机制在处理长序列时，很容易丢失序列开头的关键信息。来源：docs/chapter4/11_attention.md 第3-4行。

8、 [单选] LoRA技术通过什么方式大幅减少需要优化的参数量？
 A：删除模型中大部分权重矩阵
 B：用两个低秩矩阵A和B的乘积来模拟庞大的更新矩阵ΔW
 C：只训练模型的输出层
 D：降低整个模型的维度
正确答案：B 你的答案：B
 解析：LoRA的核心思想是用两个更小的"低秩"矩阵A和B的乘积，来模拟(近似)这个庞大的更新矩阵ΔW。通过这种方式，需要优化的参数量从d×k下降到了d×r+r×k。通常秩r会选择一个非常小的值(如8,16,64)，使得可训练参数量仅为全量微调的千分之一甚至万分之一。来源：docs/chapter11/02_lora.md 第11-19行和第35行。

9、 [单选] 当模型缺乏特定或实时知识而无法回答时，应该采用什么技术为模型提供上下文信息？
 A：提示词工程
 B：检索增强生成(RAG)
 C：全量微调
 D：模型量化
正确答案：B 你的答案：B
 解析：如果模型缺乏特定或实时知识而无法回答，下一步尝试使用RAG，通过外挂知识库为大模型提供上下文信息。这两种方法(提示工程和RAG)的核心是"引导"和"提供知识"，能解决大部分问题。来源：docs/chapter11/04_qwen2.5_qlora.md 第579行。

10、 [单选] 微调(Fine-tuning)的主要目标是什么？
 A：灌输新知识
 B：改变模型的"行为模式"或传授"技能"
 C：让模型学会聊天
 D：降低模型的参数量
正确答案：B 你的答案：B
 解析：微调通过在高质量示例上继续训练，直接修改模型权重，从根本上重塑其能力。它并非用于灌输新知识(这是RAG的强项)，而是用于传授特定的"技能"或"行为模式"。来源：docs/chapter11/04_qwen2.5_qlora.md 第581行。

11、 [单选] 2020年GPT-3论文提出的一种无需训练的范式，通过在输入中提供任务示例就能引导大模型完成特定任务，这种范式叫什么？
 A：迁移学习
 B：参数高效微调(PEFT)
 C：上下文学习(In-Context Learning)
 D：强化学习
正确答案：C 你的答案：C
 解析：2020年GPT-3论文带来了一种全新的、无需训练的范式——In-Context Learning(上下文学习)。研究者们发现，在不调整任何模型参数的情况下，仅通过在输入中提供一些任务示例(即提示Prompt)，就能引导大模型完成特定任务。来源：docs/chapter11/01_PEFT.md 第18行。

12、 [单选] 注意力机制的核心思想是什么？
 A：将所有输入信息压缩成一个固定长度的向量
 B：建立输入和输出之间的固定对齐关系
 C：动态地为输入序列的每个部分分配不同的注意力权重，生成专属当前时间步的上下文向量
 D：随机选择输入序列的部分信息
正确答案：C 你的答案：C
 解析：注意力机制的原理是在解码器生成每一个词元时，不再依赖一个固定的上下文向量，而是允许它"回头看"一遍完整的输入序列，并根据当前解码的需求，自主地为输入序列的每个部分分配不同的注意力权重，然后基于这些权重将输入信息加权求和，生成一个动态的、专属当前时间步的上下文向量。来源：docs/chapter4/11_attention.md 第16-17行。

13、 [单选] "硬提示"(Hard Prompt)这种方法存在的主要局限是什么？
 A：无法生成文本
 B：需要大量的试错和经验，过程繁琐且不稳定
 C：会改变模型权重
 D：需要大量训练数据
正确答案：B 你的答案：B
 解析：找到最优的提示词往往需要大量的试错和经验，过程繁琐且不稳定，充满了"玄学"；离散的文本提示在表达能力上存在上限，难以充分激发和精确控制大模型的潜力。来源：docs/chapter11/01_PEFT.md 第20行。

14、 [单选] PEFT(Parameter-Efficient Fine-Tuning)的核心思想是什么？
 A：修改模型99%以上的参数
 B：冻结预训练模型99%以上的参数，仅调整极小一部分或增加一些额外的小参数
 C：重新训练整个模型
 D：随机初始化所有模型参数
正确答案：B 你的答案：B
 解析：PEFT的核心思想是冻结(freeze)预训练模型99%以上的参数，仅调整其中极小一部分(通常<1%)的参数，或者增加一些额外的"小参数"，从而以极低的成本让模型适应下游任务。来源：docs/chapter11/01_PEFT.md 第26行。

15、 [单选] OWASP 2025版新增的两个直接回应社区对RAG架构和提示工程安全性迫切需求的条目是什么？
 A：提示注入和供应链漏洞
 B：System Prompt Leakage(系统提示泄露)和Vector and Embedding Weaknesses(向量与嵌入弱点)
 C：过度代理和虚假信息
 D：数据与模型投毒
正确答案：B 你的答案：B
 解析：相比于2023年的初始版本，2025版列表通过引入"System Prompt Leakage"(系统提示泄露)和"Vector and Embedding Weaknesses"(向量与嵌入弱点)等新条目，直接回应了社区对RAG架构和提示工程安全性的迫切需求。来源：docs/chapter16/02_threat_modeling_analysis.md 第7行。

16、 [单选] Adapter Tuning方法是在Transformer的每个块中插入什么模块？
 A：低秩矩阵
 B：小型"适配器"(Adapter)模块
 C：卷积层
 D：注意力层
正确答案：B 你的答案：B
 解析：Adapter Tuning的思路是在Transformer的每个块中插入小型的"适配器"(Adapter)模块。Adapter被插入到每个子层(注意力层和前馈网络)的内部，并与主干网络形成残差连接。来源：docs/chapter11/01_PEFT.md 第34-35行。

17、 [单选] 在RLHF(基于人类反馈的强化学习)中，奖励模型(Reward Model)的任务是什么？
 A：生成文本
 B：输入(prompt, response)，输出一个标量分数reward
 C：训练基础语言模型
 D：收集人类标注数据
正确答案：B 你的答案：B
 解析：奖励模型(RM)通常和我们正在优化的语言模型结构类似(但可以小得多)，它的任务不是生成文本，而是输入(prompt, response)，输出一个标量分数reward。来源：docs/chapter12/01_RLHF.md 第127行。

18、 [单选] LoRA相比Adapter Tuning的最大优势是什么？
 A：参数更多
 B：零额外推理延迟
 C：训练速度更快
 D：效果更好
正确答案：B 你的答案：B
 解析：这是LoRA相比Adapter Tuning最具吸引力的优点。Adapter在模型中串行地引入了新的计算层，不可避免地会增加推理延迟。而LoRA的旁路结构在训练完成后，可以通过矩阵加法直接"合并"回原始权重中，模型的网络结构与原始模型完全一致，不会引入任何额外的计算步骤。来源：docs/chapter11/02_lora.md 第50-51行。

19、 [单选] 多智能体交互(Multi-Agent Interaction)会引入什么新的风险？
 A：数据泄露
 B：多个AI智能体之间的自主交互可能产生不可预测的反馈循环，导致系统性的失控
 C：模型无法生成文本
 D：增加训练成本
正确答案：B 你的答案：B
 解析：多智能体交互(Multi-Agent Interaction)也引入了新的风险，多个AI智能体之间的自主交互可能产生不可预测的反馈循环，导致系统性的失控。来源：docs/chapter16/01_LLM_safety_overview.md 第37行。

20、 [单选] 自注意力机制和交叉注意力机制的主要区别是什么？
 A：自注意力用于对齐和整合两个不同序列之间的信息，交叉注意力用于理解和重构单个序列内部的依赖关系
 B：交叉注意力用于对齐和整合两个不同序列之间的信息，自注意力用于理解和重构单个序列内部的依赖关系
 C：自注意力使用Query、Key、Value，交叉注意力只使用Query
 D：没有区别，功能完全相同
正确答案：B 你的答案：B
 解析：交叉注意力用于对齐和整合两个不同序列之间的信息。而自注意力则用于理解和重构单个序列内部的依赖关系。尽管底层的加权求和计算方式相似，但两者在架构上的目标完全不同。来源：docs/chapter4/12_transformer.md 第48-51行。

21、 [单选] 提示注入位列OWASP Top 10 for LLM Applications之首，它利用的是大语言的什么特性进行攻击？
 A：参数高效微调特性
 B："指令遵循"特性进行的语义攻击
 C：注意力机制
 D：Transformer架构
正确答案：B 你的答案：B
 解析：提示注入位列OWASP Top 10 for LLM Applications(2025)之首，它不是简单的技术漏洞，而是利用大语言模型"指令遵循"特性进行的语义攻击。当攻击者构造的输入在语义权重上压倒了系统预设的指令时，模型便会发生"倒戈"，执行非预期的操作。来源：docs/chapter16/02_threat_modeling_analysis.md 第54-55行。

22、 [单选] LoRA中的缩放因子s通常设为什么？
 A：r/α
 B：α/r
 C：α*r
 D：1
正确答案：B 你的答案：B
 解析：LoRA的前向计算公式会包含一个缩放因子s：h = W₀·x + s·(B·A)·x。这个s通常设为α/r，其中α是一个可调超参。这个缩放操作有助于在调整秩r时，减少对学习率等其他超参数的重新调整需求。来源：docs/chapter11/02_lora.md 第40行。

23、 [单选] 向量与嵌入弱点(Vector and Embedding Weaknesses)主要针对的是什么架构？
 A：RNN架构
 B：RAG架构
 C：Transformer架构
 D：CNN架构
正确答案：B 你的答案：B
 解析：向量与嵌入弱点是针对RAG架构的新兴风险，涉及向量数据库和Embedding生成过程。主要风险点包括向量投毒、嵌入逆向和权限失效。来源：docs/chapter16/02_threat_modeling_analysis.md 第39-42行。

24、 [单选] 在自注意力计算中，输入序列中的每个词元会生成哪三个向量？
 A：Input、Output、Hidden
 B：Query、Key、Value
 C：Encoder、Decoder、Attention
 D：Weight、Bias、Gradient
正确答案：B 你的答案：B
 解析：对于输入序列中的每一个词元，首先获取其词嵌入向量xi。然后，将该向量分别与三个可学习的、在整个模型中共享的权重矩阵WQ、WK、WV相乘，生成该词元专属的Query向量qi、Key向量ki和Value向量vi。来源：docs/chapter4/12_transformer.md 第59行。

25、 [单选] 提示词工程和RAG在"LLM优化"维度上的共同特点是什么？
 A：都会修改模型权重
 B：完全不改变模型权重
 C：需要大量训练数据
 D：增加模型参数
正确答案：B 你的答案：B
 解析：横轴代表"LLM优化"，即对模型本身进行多大程度的修改。从左到右，优化的程度越来越深，其中提示工程和RAG完全不改变模型权重，而微调则直接修改模型参数。来源：docs/chapter11/04_qwen2.5_qlora.md 第571行。

26、 [单选] 当训练LoRA时，哪些参数会被更新？
 A：原始预训练权重W₀
 B：旁路的低秩矩阵A和B
 C：所有模型参数
 D：只有偏置参数
正确答案：B 你的答案：B
 解析：在训练时，只有旁路的矩阵A和B会被更新。通过这种方式，需要优化的参数量就从d×k下降到了d×r+r×k。来源：docs/chapter11/02_lora.md 第35行。

27、 [单选] DAN(Do Anything Now)攻击属于哪种类型的提示注入？
 A：间接提示注入
 B：直接提示注入(越狱)
 C：系统提示泄露
 D：向量投毒
正确答案：B 你的答案：B
 解析：直接提示注入也就是"越狱"。攻击者直接与LLM对话，利用"角色扮演"或逻辑陷阱，诱导模型输出本应被屏蔽的有害内容。最为知名的便是DAN(Do Anything Now)模式，攻击者通过构建一个名为"DAN"的虚拟角色，明确告知模型该角色"不受任何规则限制"。来源：docs/chapter16/02_threat_modeling_analysis.md 第9-10行和第76行。

28、 [单选] "幻觉"(Hallucination)指模型在什么情况下仍然以自信的口吻编造看似合理但事实错误的内容？
 A：输入过短时
 B：在缺乏真实依据或检索失败时
 C：训练数据过多时
 D：使用提示词工程时
正确答案：B 你的答案：B
 解析："幻觉"指模型在缺乏真实依据或检索失败时，仍然以自信的口吻编造看似合理但事实错误的内容，是当前大模型可信度的核心瓶颈之一。幻觉并非仅由推理阶段的随机性导致，而是贯穿于大模型研发的全生命周期。来源：docs/chapter16/01_LLM_safety_overview.md 第73-75行。

29、 [单选] 在有监督指令微调(SFT)阶段，使用的数据格式通常是怎样的？
 A：(prompt, response, reward)
 B：(prompt, response)
 C：(input, output, label)
 D：(sequence, probability)
正确答案：B 你的答案：B
 解析：有监督指令微调使用少量高质量"指令-回答"对，对基础模型进行有监督微调，使其学会理解并执行人类指令。这一阶段通常使用(prompt, response)格式的数据。来源：docs/chapter12/01_RLHF.md 第71行。

30、 [单选] 微调的主要适用场景是什么？
 A：模型缺乏知识时
 B：改变模型的"行为模式"、传授"技能"(如遵循特定格式、模仿特定风格)时
 C：简单任务场景
 D：降低推理延迟
正确答案：B 你的答案：B
 解析：当上述方法都无法满足需求时，就需要成本较高也是效果最好的微调登场。微调并非用于灌输新知识(这是RAG的强项)，而是用于传授特定的"技能"或"行为模式"。只有当目标是改变模型的"行为模式"、传授"技能"时，才是微调登场的最佳时机。来源：docs/chapter11/04_qwen2.5_qlora.md 第581行和第595行。

多选题 （每题1分，共10道题）
1、 [多选] OWASP Top 10 for LLM Applications(2025)中，过度代理(Over-delegation)风险的具体表现形式包括哪些？
 A：功能过度：如邮件插件被赋予"删除"权限，而不仅仅是"读取"
 B：权限过度：使用Root/Admin身份连接数据库
 C：自主过度：高风险操作(如转账)缺乏"人在回路"确认
 D：系统提示泄露
正确答案：A,B,C 你的答案：A,B,C
 解析：过度代理是指赋予LLM Agent过高的权限、功能或自主权，导致其在幻觉或受攻击时造成实质性破坏。具体表现形式包括：功能过度、权限过度、自主过度。来源：docs/chapter16/02_threat_modeling_analysis.md 第30-33行。

2、 [多选] 以下哪些是LoRA相比Adapter Tuning和Prompt Tuning的优势？
 A：更高的参数与存储效率，可以将模型checkpoints的体积缩小高达10,000倍
 B：零额外推理延迟，可以通过矩阵加法直接合并回原始权重
 C：效果媲美全量微调，且不占用输入长度
 D：必须为不同任务使用相同的LoRA权重
正确答案：A,B,C 你的答案：A,B,C
 解析：LoRA展现出几大核心优势：更高的参数与存储效率；零额外推理延迟；效果媲美全量微调，且不占用输入长度；良好的可组合性。如果你需要为不同的任务(拥有不同的LoRA权重)同时提供服务，在单个batch中混合处理这些任务会变得不那么直接，这并非优势。来源：docs/chapter11/02_lora.md 第48-53行。

3、 [多选] 向量与嵌入弱点(Vector and Embedding Weaknesses)的主要风险点包括哪些？
 A：向量投毒：向知识库注入含恶意指令的文档，通过检索劫持模型
 B：嵌入逆向：可能从向量表示中部分还原出原始敏感文本或推断敏感属性
 C：权限失效：多租户向量检索时未做行级权限隔离
 D：直接命令模型"忘记所有规则"
正确答案：A,B,C 你的答案：A,B,C
 解析：向量与嵌入弱点涉及向量数据库和Embedding生成过程。主要风险点包括：向量投毒、嵌入逆向、权限失效。来源：docs/chapter16/02_threat_modeling_analysis.md 第39-42行。

4、 [多选] 标准Seq2Seq模型试图将源序列的所有信息无差别地压缩进一个向量时，固定对齐策略存在哪些局限性？
 A：要求序列等长
 B：对齐关系僵化
 C：无法处理中文文本
 D：需要大量计算资源
正确答案：A,B 你的答案：A,B
 解析：固定对齐策略的局限性：(1)要求序列等长：对于不等长的序列(如中英文翻译)，这种一对一的映射关系立刻失效。(2)对齐关系僵化：它假设了输入和输出的对齐关系是固定不变的，但实际任务中的对应关系可能非常复杂(如一对多、多对一)。来源：docs/chapter4/11_attention.md 第37-39行。

5、 [多选] PEFT(Parameter-Efficient Fine-Tuning)技术发展脉络中，"作用于激活值的加法思路"包括哪些方法？
 A：Adapter Tuning
 B：Prompt Tuning及其演进(Prefix-Tuning、P-Tuning v2)
 C：LoRA
 D：全量微调
正确答案：A,B 你的答案：A,B
 解析：PEFT技术发展脉络第一条线路是"作用于激活值的加法思路"：我们首先学习了以Adapter Tuning为代表的早期方法；随后，我们转向了更为非侵入式的Prompt Tuning及其演进，包括Prefix-Tuning和P-Tuning v2。LoRA属于"重参数化权重的手术刀思路"。来源：docs/chapter11/04_qwen2.5_qlora.md 第88-89行。

6、 [多选] 有监督指令微调(SFT)中使用的数据集可以分哪两类？
 A：任务型指令数据集：如WizardLM Evol-Instruct 70k和Dolly-15k
 B：对话型数据集：如OpenAssistant(OASST)
 C：奖励模型数据集
 D：预训练数据集
正确答案：A,B 你的答案：A,B
 解析：高质量的SFT数据集是模型能力养成的关键。根据目标的不同，数据集可以分为两类：任务型指令数据集和对话型数据集。来源：docs/chapter12/01_RLHF.md 第71-78行。

7、 [多选] 下列哪些情况应该考虑使用微调(Fine-tuning)？
 A：让模型学会严格遵循某种独特的输出格式(如特定的JSON结构)
 B：模仿特定人物的对话风格
 C：深度适配某个专业领域的"行话"
 D：模型缺乏特定或实时知识
正确答案：A,B,C 你的答案：A,B,C
 解析：微调用于传授特定的"技能"或"行为模式"。例如，让模型学会严格遵循某种独特的输出格式(如特定的JSON结构)、模仿特定人物的对话风格、深度适配某个专业领域的"行话"，或者将极其复杂的指令"蒸馏"进模型权重中以优化API调用。模型缺乏特定或实时知识应该使用RAG。来源：docs/chapter11/04_qwen2.5_qlora.md 第581行。

8、 [多选] 大模型安全可以从哪四个核心维度来系统性地刻画风险版图？
 A：价值对齐与有害内容
 B：可靠性与幻觉
 C：对抗攻击与滥用
 D：隐私与合规
正确答案：A,B,C,D 你的答案：A,B,C,D
 解析：我们将重点从"价值对齐与有害内容""可靠性与幻觉""对抗攻击与滥用""隐私与合规"这四个核心维度，来系统性地刻画大模型面临的具体风险版图。来源：docs/chapter16/01_LLM_safety_overview.md 第51-52行。

9、 [多选] RAG、工具调用和代理化能力让模型不再只是"会说话"，而是可以触达什么？
 A：数据库
 B：业务系统
 C：外部API
 D：内部权重矩阵
正确答案：A,B,C 你的答案：A,B,C
 解析：工业界大量使用检索增强生成(RAG)、函数调用(Tools/Plugins)和多代理编排(Agents)等技术，让模型具备"查资料、调接口、驱动系统"的能力。这些扩展让模型不再只是"会说话"，而是可以触达数据库、业务系统和外部API，因此也被视为新一代高危接口。来源：docs/chapter16/01_LLM_safety_overview.md 第34-35行。

10、 [多选] 关于技术选型决策框架"提示词工程 -> RAG -> 微调"，以下哪些说法是正确的？
 A：应优先尝试成本最低的提示词工程
 B：当模型缺乏知识时，采用RAG为模型"开卷考试"
 C：只有当目标是改变模型的"行为模式"、传授"技能"时，才是微调登场的最佳时机
 D：微调应该永远是首选方案
正确答案：A,B,C 你的答案：A,B,C
 解析：在结束本章时，我们还建立了一个技术选型的决策框架，"提示词工程 -> RAG -> 微调"。应优先尝试成本最低的提示词工程；当模型缺乏知识时，采用RAG为其"开卷考试"；只有当目标是改变模型的"行为模式"、传授"技能"(如遵循特定格式、模仿特定风格)时，才是微调登场的最佳时机。来源：docs/chapter11/04_qwen2.5_qlora.md 第595行。

判断题 （每题1分，共20道题）
1、 [判断] 提示词工程和RAG都会修改模型的权重。
正确选项：错 你的选项：错
 解析：横轴代表"LLM优化"，即对模型本身进行多大程度的修改。其中提示工程和RAG完全不改变模型权重，而微调则直接修改模型参数。来源：docs/chapter11/04_qwen2.5_qlora.md 第571行。

2、 [判断] 在RLHF的三步法中，第一步就是训练奖励模型。
正确选项：错 你的选项：错
 解析：RLHF的流程主要包含三个核心步骤。首先通过有监督微调得到初始策略模型；然后，收集人类偏好数据训练一个奖励模型；最后，使用奖励模型作为信号，通过强化学习算法(如PPO)进一步优化策略模型。第一步是SFT，不是训练奖励模型。来源：docs/chapter12/01_RLHF.md 第105-111行。

3、 [判断] LoRA会引入额外的推理延迟。
正确选项：错 你的选项：错
 解析：LoRA的旁路结构在训练完成后，可以通过矩阵加法直接"合并"回原始权重中。这样，模型的网络结构与原始模型完全一致，不会引入任何额外的计算步骤，因此是零额外推理延迟。来源：docs/chapter11/02_lora.md 第50-51行。

4、 [判断] 间接提示注入需要攻击者直接与模型对话。
正确选项：错 你的选项：错
 解析：间接提示注入是攻击者将恶意指令隐藏在网页、邮件或文档中。当RAG系统或联网模型检索并处理这些内容时，模型被"劫持"执行隐藏指令。攻击者无需直接与模型对话。来源：docs/chapter16/02_threat_modeling_analysis.md 第10-11行。

5、 [判断] 自注意力机制中，Query、Key、Value来源于同一个输入序列。
正确选项：对 你的选项：对
 解析：在自注意力机制中，Query、Key、Value均来源于同一个输入序列。其目的是为了捕捉输入序列内部的依赖关系。来源：docs/chapter4/12_transformer.md 第9-11行。

6、 [判断] LoRA的两个低秩矩阵A和B都需要使用高斯分布进行随机初始化。
正确选项：错 你的选项：错
 解析：矩阵A通常使用高斯分布进行随机初始化，而矩阵B则初始化为全零。这样做可以确保在训练开始时，旁路输出为零，微调是从原始的预训练模型状态开始的。来源：docs/chapter11/02_lora.md 第37-39行。

7、 [判断] 微调主要用于灌输新知识给模型。
正确选项：错 你的选项：错
 解析：微调通过在高质量示例上继续训练，直接修改模型权重。它并非用于灌输新知识(这是RAG的强项)，而是用于传授特定的"技能"或"行为模式"。来源：docs/chapter11/04_qwen2.5_qlora.md 第581行。

8、 [判断] Adapter Tuning不会引入推理延迟。
正确选项：错 你的选项：错
 解析：Adapter在模型中串行地引入了新的计算层，不可避免地会增加推理延迟。而LoRA的旁路结构可以合并回原始权重，不会引入任何额外的计算步骤，具有零额外推理延迟的优势。来源：docs/chapter11/02_lora.md 第50-51行。

9、 [判断] Transformer架构抛弃了RNN和卷积网络，完全基于注意力机制构建。
正确选项：对 你的选项：对
 解析：Transformer抛弃了传统的RNN和卷积网络，整个模型基于注意力机制来构建。它不仅凭借其出色的并行计算能力极大地提升了训练效率，还更有效地捕捉了文本中的长距离依赖关系。来源：docs/chapter4/12_transformer.md 第3-5行。

10、 [判断] DAN(Do Anything Now)攻击是一种间接提示注入攻击。
正确选项：错 你的选项：错
 解析：DAN(Do Anything Now)属于直接提示注入(越狱)。攻击者直接与LLM对话，利用"角色扮演"或逻辑陷阱，诱导模型输出本应被屏蔽的有害内容。来源：docs/chapter16/02_threat_modeling_analysis.md 第9-10行和第76行。

11、 [判断] 多智能体交互不会引入新的安全风险。
正确选项：错 你的选项：错
 解析：多智能体交互(Multi-Agent Interaction)也引入了新的风险，多个AI智能体之间的自主交互可能产生不可预测的反馈循环，导致系统性的失控。来源：docs/chapter16/01_LLM_safety_overview.md 第37行。

12、 [判断] 提示词工程是按照成本效益原则，优先级最低的方案。
正确选项：错 你的选项：错
 解析：整个技术选型路径应遵循成本效益原则，从低到高依次为"提示词工程(Prompt Engineering) -> 检索增强生成(RAG) -> 微调(Fine-tuning)"。提示词工程是成本最低、优先级最高的方案。来源：docs/chapter11/04_qwen2.5_qlora.md 第569行。

13、 [判断] 标准Seq2Seq模型在处理长序列时不会丢失信息。
正确选项：错 你的选项：错
 解析：标准Seq2Seq架构存在一个核心缺陷：信息瓶颈。编码器需要将源序列的所有信息，不论长短，全部压缩成一个固定长度的上下文向量。这种机制在处理长序列时，很容易丢失序列开头的关键信息。来源：docs/chapter4/11_attention.md 第3-4行。

14、 [判断] LoRA的秩r通常会设置为一个较大的值(如1000)以获得更好的效果。
正确选项：错 你的选项：错
 解析：通常，秩r会选择一个非常小的值(如8,16,64)，使得可训练参数量仅为全量微调的千分之一甚至万分之一。来源：docs/chapter11/02_lora.md 第35行。

15、 [判断]幻觉问题仅由推理阶段的随机性导致。
正确选项：错 你的选项：错
 解析："幻觉"指模型在缺乏真实依据或检索失败时，仍然以自信的口吻编造看似合理但事实错误的内容。幻觉并非仅由推理阶段的随机性导致，而是贯穿于大模型研发的全生命周期。来源：docs/chapter16/01_LLM_safety_overview.md 第73-75行。

16、 [判断] 在自注意力机制中，解码器生成每一个词元时都依赖一个固定的上下文向量。
正确选项：错 你的选项：错
 解析：注意力机制的原理是在解码器生成每一个词元时，不再依赖一个固定的上下文向量，而是允许它"回头看"一遍完整的输入序列，并根据当前解码的需求，自主地为输入序列的每个部分分配不同的注意力权重，生成一个动态的上下文向量。来源：docs/chapter4/11_attention.md 第16-17行。

17、 [判断] PEFT的核心思想是冻结预训练模型99%以上的参数，仅调整极小一部分参数或增加小参数。
正确选项：对 你的选项：对
 解析：PEFT的核心思想是冻结(freeze)预训练模型99%以上的参数，仅调整其中极小一部分(通常<1%)的参数，或者增加一些额外的"小参数"，从而以极低的成本让模型适应下游任务。来源：docs/chapter11/01_PEFT.md 第26行。

18、 [判断] 对话型数据集OpenAssistant(OASST)专注于教会模型如何"做事"。
正确选项：错 你的选项：错
 解析：简单来说就是任务型数据集教会模型"智商"，而以OASST为代表的对话型数据集则赋予模型"情商"，使模型更接近一个真正能与人交流的智能助手。来源：docs/chapter12/01_RLHF.md 第80行。

19、 [判断] LoRA可以直接作用于模型权重，而不需要通过影响激活值来适应新任务。
正确选项：对 你的选项：对
 解析：LoRA不再"绕道而行"，而是直击模型的权重矩阵，并通过低秩分解来模拟权重更新矩阵，实现了性能与效率的平衡。来源：docs/chapter11/02_lora.md 第5行。

20、 [判断] RAG的主要目的是改变模型的"行为模式"。
正确选项：错 你的选项：错
 解析：RAG通过外挂知识库为大模型提供上下文信息。这两种方法(提示工程和RAG)的核心是"引导"和"提供知识"，能解决大部分问题，但无法从根本上改变模型的"行为"或"技能"。微调才是用于改变模型"行为模式"的技术。来源：docs/chapter11/04_qwen2.5_qlora.md 第579-581行。
